---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---
### Load necessary libraries
```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels","stringr","maps","countrycode","rworldmap")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("tidytext")
library("ggplot2")
library("stringr")
library("maps")
library("countrycode")
library("rworldmap")

source("../lib/plotstacked.R") # not used
source("../lib/speechFuncs.R") # not used
```
### Get meta data from xlsx file
```{r}
library("xlsx")
library("rJava")
library("xlsxjars")
inaug =read.xlsx(file.path("..","data","InaugurationInfo.xlsx",fsep = .Platform$file.sep),1)
inaug$File = NULL
inaug$Words = NULL
```
### Let's get our inaugural speeches into a corpus
```{r}
library(tm)
ds = DirSource(directory = file.path("..","data","InauguralSpeeches",fsep = .Platform$file.sep),
               pattern = "*.txt",
               mode = "text")
corpus = VCorpus(ds,readerControl = list(reader=ds$reader,language="en"))
```
# Check to ensure that speeches were correctly imported
```{r}
corpus[[1]]$content
```
# Convert speeches into a Tibble data frame
```{r}
# loop over corpus to get 
rawtext = NULL
for(i in seq(length(corpus))) {
  rawtext[i] <- corpus[[i]]$content
}

# create a tibble
tb_speeches = tibble(author=inaug$President,
            party=inaug$Party,
            text=rawtext,
            term=inaug$Term)
```
### For the first task, let's split the speeches into sentences and try to figure out whether the speeches are more "positive" or "negative" on balance.  We split the text into sentences because whole paragraphs tend to average out positive and negative sentiments.  We'll assign sentiments to sentences using the NRC method. 
```{r}
tb_sentences <- tb_speeches %>%
  group_by(author) %>%
  unnest_tokens(sentence, text, token = "sentences") 
tb_sentences$score <- get_sentiment(tb_sentences$sentence, method = "nrc")
```
### Let's see which Presidents have the highest mean sentiment scores (calculated over the sentences in their inaugural address) based on the NRC scoring method.
```{r}
mean_scores <- tb_sentences %>%
  group_by(author) %>%
  summarise(mean(score))
mean_scores$score <- mean_scores$`mean(score)` # change annoying column name.  There's prob a better way
mean_scores$`mean(score)` <- NULL

ggplot(mean_scores,aes(author,score)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(y = "Mean Sentiment Over Sentences in Inaugural Address",
       x = NULL) +
  coord_flip()
# ideally would like to be able to sort by the top sentiment scores to the bottom
```
### One interesting observation about the sentiment measure is that Trump is assigned a highly positive sentiment even though some reports of his speech characterized it as one of the gloomiest speeches of all time.  It might be because Trump has a tendency to use positive superlatives in a more indiscriminate manner compared to other, more tempered politicans.

### Now we will look at word counts to see if there are any interesting trends to observe there.  Not surprisingly, the word cloud shows that most of the words have to do with politics.
```{r, warning=FALSE}
stop_words <- data("stop_words")
tb_words <- tb_speeches %>%
  group_by(author) %>%
  unnest_tokens(word, text, token = "words") 
tb_words$word <- removePunctuation(tb_words$word)
tb_words$word <- removeNumbers(tb_words$word)
data(stop_words)
tb_words_nostop <- anti_join(tb_words,stop_words,by=c("word"="word"))

wordcloud <- tb_words_nostop %>%
  group_by(word) %>%
  count(word,sort=TRUE)
wordcloud(wordcloud$word,wordcloud$n,max.words=50)
```
# Let's confirm our understanding of the topic of "politics" by running an LDA (removing all stop words)
```{r}
dtm <- DocumentTermMatrix(corpus, control = list(removePunctuation = TRUE, stopwords = TRUE))
lda <- LDA(dtm,method="Gibbs",control = list(alpha=0.1),k=5) # running only a 5 topic model
topics <- topics(lda,5)
```
# Let's view some of the topics derived from the lda model.  These are topics provided by the LDA model, and match what we know about American politics.  It would be interesting to compare topic models with other countries, for example, China where there are different political buzzwords like "harmonious society".
```{r}
topics.hash
```
# Let's go back to the most frequent terms (excluding stop words).  I'm curious to know more about how president's differ from each other in their topic choice.
```{r}
tb_words_nostop %>%
  group_by(word) %>%
  count(word, sort=TRUE)
```
# Let's visualize that
```{r}
tb_words_nostop %>%
  count(word, sort = TRUE) %>%
  filter(n > 150) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  xlab(NULL) +
  coord_flip()
```

# Now let's see how Obama and Trump compare
```{r}



tidy_comparison <- bind_rows(trump <- filter(tb_words_nostop, author=="Donald J. Trump"),
                             clinton <- filter(tb_words_nostop, author=="William J. Clinton"))

obama_percent <- tb_words_nostop %>%
  mutate(word = str_extract(words, "[a-z']+")) %>%
  count(word) %>%
  transmute(word, obama = n / sum(n))

frequency <- tidy_comparison %>%
  mutate(word = str_extract(words, "[a-z']+")) %>%
  count(author, word) %>%
  mutate(other = n / sum(n)) %>%
  left_join(obama_percent) %>%
  ungroup()
```
# make the plot showing relative word frequencies between Trump and Obama and Nixon and Obama
```{r}
library(scales)

ggplot(frequency, aes(x = other, y = obama, color = abs(obama - other))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Barack Obama", x = NULL)
```

# Let's examine the correlation between Obama and Trump and Obama and Clinton
```{r}
cor.test(data = frequency[frequency$author == "Donald J. Trump",],
         ~ other + obama)
cor.test(data = frequency[frequency$author == "William J. Clinton",], 
         ~ other + obama)
```
# It turns out the correlation between obama and trump is somewhat lower than between obama and clinton.  It's interesting to note that Obama and Clinton seem to mention themes like the "world" and "democracy" equally frequently

# Now let's look at some bigrams to see if there are any phrases that appear often
```{r}
clean_tb<-tb
clean_tb$text <-removePunctuation(tb$text,preserve_intra_word_dashes = TRUE)
clean_tb$text <-removeNumbers(tb$text)
clean_tb$text <-removeWords(tb$text,stopwords("english"))

word_bigram <- clean_tb %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

word_bigram
```
# Let's look at the most common bigrams
```{r}
word_bigram %>%
  count(bigram, sort = TRUE)
```
# There are a lot of "we"" and "I" constructions - this makes sense for a political speech.  What would happen if we could separate out these more generic phrases to see if there were more meaningful bigrams like "islamic terrorism"?

```{r, message=FALSE, warning=FALSE}
require(tidyr)
bigrams_separated <- word_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```
# There are some notable associations, like "protect, defend" and "constitution, laws" - this is perhaps due to the fact that these words are contained in the oath of office, which is included in some of the texts.

# Now we calculate tf-idf on bigrams for different parties to give us an idea of their most unique phrases (ones that are closely associated only with them)
```{r bigram_tf_idf, dependson = "bigram_counts"}
bigram_tf_idf <- bigrams_united %>%
  count(party, bigram) %>%
  bind_tf_idf(bigram, party, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```

# Let's plot differences in parties to see if there are any interesting trends.

```{r}

bigram_tf_idf_parties <- bind_rows(a <- filter(bigram_tf_idf, party=="Republican"),
                                   b <- filter(bigram_tf_idf, party=="Democratic"))

bigram_tf_idf_parties %>%
  arrange(desc(tf_idf)) %>%
  top_n(12, tf_idf) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram, tf_idf, fill = party)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ party, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of bigram by party",
       x = "")
```
# These results are a little disappointing - from the plot we can see that there are a lot of bigrams that appear only once.  However, we have removed common words like "and", which has limited our ability to find matching terms.  What if we looked only for trigrams that had the structure [word] "and" [word]?  Maybe this would reveal difference between the parties in how they link concepts together.

# Let's construct the trigrams:
```{r}
trigram_tb<-tb
trigram_tb$text <-removePunctuation(tb$text,preserve_intra_word_dashes = TRUE)
trigram_tb$text <-removeNumbers(tb$text)

word_trigram <- trigram_tb %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

trigrams_separated <- word_trigram %>%
  separate(trigram, c("word1", "word2","word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(word2 %in% "and")

trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ")

# new trigram counts
trigram_counts <- trigrams_filtered %>% 
  count(word1, word2, word3, sort = TRUE)

trigram_tf_idf <- trigrams_united %>%
  count(party, trigram) %>%
  bind_tf_idf(trigram, party, n) %>%
  arrange(desc(tf_idf))

trigram_tf_idf_parties <- bind_rows(a <- filter(trigram_tf_idf, party=="Republican"),
                                   b <- filter(trigram_tf_idf, party=="Democratic"))

trigram_tf_idf_parties %>%
  arrange(desc(tf_idf)) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  mutate(trigram = reorder(trigram, tf_idf)) %>%
  ggplot(aes(trigram, tf_idf, fill = party)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ party, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of trigram by party",
       x = "")
```
# let's look at "future" words to see if that reveals any difference in which words are used to convey positive sentiments.

```{r}

AFINN <- get_sentiments("afinn")

negation_words <- c("will", "shall", "never", "ever")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()

negation_words
```

```{r}
negated_words %>%
  mutate(contribution = n * score) %>%
  mutate(word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
  group_by(word1) %>%
  top_n(5, abs(contribution)) %>%
  ggplot(aes(word2, contribution, fill = n * score > 0)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation term") +
  ylab("Sentiment score * # of occurrences") +
  coord_flip()
```
# Now let's look at which foreign countries have been mentioned in inaugural addresses - surprisingly few!
```{r}
require('maps')
require('countrycode')
require('rworldmap')

countries = tolower(unique(world.cities$country.etc)) # get country names

combined_speeches = PlainTextDocument(speech.corpus,language = "eng")
regions = c("europe","asia","africa","north america","south america","middle east","central america")

tf.countries = termFreq(combined_speeches,control = list(dictionary = countries))
tf.regions = termFreq(combined_speeches,control = list(dictionary = regions))

countries_mentioned <- names(as.list(tf.countries[tf.countries!=0]))

theCountries <- countrycode(countries_mentioned,'country.name','iso3c')
# These are the ISO3 names of the countries you'd like to plot in red

menDF <- data.frame(country = theCountries,
  mentioned = TRUE)

menMap <- joinCountryData2Map(menDF, joinCode = "ISO3",
  nameJoinColumn = "country")
# This will join your menDF data.frame to the country map data

mapCountryData(menMap, nameColumnToPlot="mentioned", catMethod = "categorical",
  missingCountryCol = gray(.8), mapTitle = "Foreign Countries Mentioned in Inaugural Addresses", addLegend = FALSE)
# And this will plot it, with the trick that the color palette's first
# color is red
```

